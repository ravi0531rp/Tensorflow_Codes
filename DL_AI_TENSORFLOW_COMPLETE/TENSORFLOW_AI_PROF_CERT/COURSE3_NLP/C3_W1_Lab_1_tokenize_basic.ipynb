{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C3_W1_Lab_1_tokenize_basic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX4Kg8DUTKWO"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX8mhOLljYeM"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "BZSlp3DAjdYf"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkVZSkIyQIQX"
      },
      "source": [
        "**Note:** This notebook can run using TensorFlow 2.5.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdVrYNXHQIQX"
      },
      "source": [
        "#!pip install tensorflow==2.5.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaCMcjMQifQc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9535993d-1fed-474d-f13e-a7e40f2a0785"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat',\n",
        "    'You love my dog!'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X_9I5fXQIQY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fc61efb-dc5c-480a-b0ff-81efe4c99733"
      },
      "source": [
        "type(word_index)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i51z_yxmzIIm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0e24068-3a29-47df-c331-2ff796c42a0f"
      },
      "source": [
        "help(tokenizer)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on Tokenizer in module keras_preprocessing.text object:\n",
            "\n",
            "class Tokenizer(builtins.object)\n",
            " |  Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ', char_level=False, oov_token=None, document_count=0, **kwargs)\n",
            " |  \n",
            " |  Text tokenization utility class.\n",
            " |  \n",
            " |  This class allows to vectorize a text corpus, by turning each\n",
            " |  text into either a sequence of integers (each integer being the index\n",
            " |  of a token in a dictionary) or into a vector where the coefficient\n",
            " |  for each token could be binary, based on word count, based on tf-idf...\n",
            " |  \n",
            " |  # Arguments\n",
            " |      num_words: the maximum number of words to keep, based\n",
            " |          on word frequency. Only the most common `num_words-1` words will\n",
            " |          be kept.\n",
            " |      filters: a string where each element is a character that will be\n",
            " |          filtered from the texts. The default is all punctuation, plus\n",
            " |          tabs and line breaks, minus the `'` character.\n",
            " |      lower: boolean. Whether to convert the texts to lowercase.\n",
            " |      split: str. Separator for word splitting.\n",
            " |      char_level: if True, every character will be treated as a token.\n",
            " |      oov_token: if given, it will be added to word_index and used to\n",
            " |          replace out-of-vocabulary words during text_to_sequence calls\n",
            " |  \n",
            " |  By default, all punctuation is removed, turning the texts into\n",
            " |  space-separated sequences of words\n",
            " |  (words maybe include the `'` character). These sequences are then\n",
            " |  split into lists of tokens. They will then be indexed or vectorized.\n",
            " |  \n",
            " |  `0` is a reserved index that won't be assigned to any word.\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ', char_level=False, oov_token=None, document_count=0, **kwargs)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit_on_sequences(self, sequences)\n",
            " |      Updates internal vocabulary based on a list of sequences.\n",
            " |      \n",
            " |      Required before using `sequences_to_matrix`\n",
            " |      (if `fit_on_texts` was never called).\n",
            " |      \n",
            " |      # Arguments\n",
            " |          sequences: A list of sequence.\n",
            " |              A \"sequence\" is a list of integer word indices.\n",
            " |  \n",
            " |  fit_on_texts(self, texts)\n",
            " |      Updates internal vocabulary based on a list of texts.\n",
            " |      \n",
            " |      In the case where texts contains lists,\n",
            " |      we assume each entry of the lists to be a token.\n",
            " |      \n",
            " |      Required before using `texts_to_sequences` or `texts_to_matrix`.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          texts: can be a list of strings,\n",
            " |              a generator of strings (for memory-efficiency),\n",
            " |              or a list of list of strings.\n",
            " |  \n",
            " |  get_config(self)\n",
            " |      Returns the tokenizer configuration as Python dictionary.\n",
            " |      The word count dictionaries used by the tokenizer get serialized\n",
            " |      into plain JSON, so that the configuration can be read by other\n",
            " |      projects.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A Python dictionary with the tokenizer configuration.\n",
            " |  \n",
            " |  sequences_to_matrix(self, sequences, mode='binary')\n",
            " |      Converts a list of sequences into a Numpy matrix.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          sequences: list of sequences\n",
            " |              (a sequence is a list of integer word indices).\n",
            " |          mode: one of \"binary\", \"count\", \"tfidf\", \"freq\"\n",
            " |      \n",
            " |      # Returns\n",
            " |          A Numpy matrix.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: In case of invalid `mode` argument,\n",
            " |              or if the Tokenizer requires to be fit to sample data.\n",
            " |  \n",
            " |  sequences_to_texts(self, sequences)\n",
            " |      Transforms each sequence into a list of text.\n",
            " |      \n",
            " |      Only top `num_words-1` most frequent words will be taken into account.\n",
            " |      Only words known by the tokenizer will be taken into account.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          sequences: A list of sequences (list of integers).\n",
            " |      \n",
            " |      # Returns\n",
            " |          A list of texts (strings)\n",
            " |  \n",
            " |  sequences_to_texts_generator(self, sequences)\n",
            " |      Transforms each sequence in `sequences` to a list of texts(strings).\n",
            " |      \n",
            " |      Each sequence has to a list of integers.\n",
            " |      In other words, sequences should be a list of sequences\n",
            " |      \n",
            " |      Only top `num_words-1` most frequent words will be taken into account.\n",
            " |      Only words known by the tokenizer will be taken into account.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          sequences: A list of sequences.\n",
            " |      \n",
            " |      # Yields\n",
            " |          Yields individual texts.\n",
            " |  \n",
            " |  texts_to_matrix(self, texts, mode='binary')\n",
            " |      Convert a list of texts to a Numpy matrix.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          texts: list of strings.\n",
            " |          mode: one of \"binary\", \"count\", \"tfidf\", \"freq\".\n",
            " |      \n",
            " |      # Returns\n",
            " |          A Numpy matrix.\n",
            " |  \n",
            " |  texts_to_sequences(self, texts)\n",
            " |      Transforms each text in texts to a sequence of integers.\n",
            " |      \n",
            " |      Only top `num_words-1` most frequent words will be taken into account.\n",
            " |      Only words known by the tokenizer will be taken into account.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          texts: A list of texts (strings).\n",
            " |      \n",
            " |      # Returns\n",
            " |          A list of sequences.\n",
            " |  \n",
            " |  texts_to_sequences_generator(self, texts)\n",
            " |      Transforms each text in `texts` to a sequence of integers.\n",
            " |      \n",
            " |      Each item in texts can also be a list,\n",
            " |      in which case we assume each item of that list to be a token.\n",
            " |      \n",
            " |      Only top `num_words-1` most frequent words will be taken into account.\n",
            " |      Only words known by the tokenizer will be taken into account.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          texts: A list of texts (strings).\n",
            " |      \n",
            " |      # Yields\n",
            " |          Yields individual sequences.\n",
            " |  \n",
            " |  to_json(self, **kwargs)\n",
            " |      Returns a JSON string containing the tokenizer configuration.\n",
            " |      To load a tokenizer from a JSON string, use\n",
            " |      `keras.preprocessing.text.tokenizer_from_json(json_string)`.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          **kwargs: Additional keyword arguments\n",
            " |              to be passed to `json.dumps()`.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A JSON string containing the tokenizer configuration.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUNPyYMXznDq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}