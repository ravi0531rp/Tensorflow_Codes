{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Input , Dense,Dropout,GlobalMaxPool2D , Conv2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "cifar_data = tf.keras.datasets.cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train) , (X_test,y_test) = cifar_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1bb679d8550>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAe80lEQVR4nO2dXWyc53Xn/2e+OENy+CV+SKJky5Y/1k5iy45qGHa3m2x2CzcomuQi2eai8EVQ9aIBGqC9MLLAJnuXFk2KXCwCKBu37iKbJmiSxiiMbbNGA6NNkLUcO/6uLcuy9UFTlEiKM5zhfJ694BiVnef/kBbJoZLn/wMEjt7D533P+8x73nfm+fOcY+4OIcSvPpnddkAI0R8U7EIkgoJdiERQsAuRCAp2IRJBwS5EIuS2MtjMHgDwVQBZAP/T3b8U+/18Pu8DxWLQ1ul06LgMwvJg1vixCjl+H8tHbLlsltrMwgc0i9wzIz622/ycY4JoNuYjkVK73uXH6vKjWSZyAhG63fC5xXyP7i/iv0UmmdkyET+yGf5+smsAALoRGdtjFwIbE91fmMXlCqq1teDBrjrYzSwL4H8A+M8AzgJ40swedfcX2ZiBYhFH7v5g0La8vEiPNZAJv9ETBT4Z1+0ZpLapiSFqmxwbprZCNh/cnhso0THI8ileXFqmtmabn9v42Ci1ZTqt4PZGo0HHrK2tUVuxFL45A0AH/GZVq1eD20fHRugYON9fs9GktizC7wvAby7lYf4+Dw3x6yOf5/NRj/josQdCJnyNxM657eGbx59+47v8MNyDDbkHwEl3P+XuTQB/A+BjW9ifEGIH2UqwzwI4c8X/z/a2CSGuQbbynT30OeIXPnua2TEAxwBgYGBgC4cTQmyFrTzZzwI4eMX/DwA4/+5fcvfj7n7U3Y/m8vy7lRBiZ9lKsD8J4GYzu8HMCgB+F8Cj2+OWEGK7ueqP8e7eNrPPAvgHrEtvD7v7C7Exa2treOHF8K8sX7xIx02QBVDbw1dGJztlarPSNLWtdrkqUO2EV8jdCnRMbY2vqNbqfIW81eFS08WI5ljMhX1st/n+smQ1GIh/9aqtrVJbuxs+b1vbQ8dkIqpcK6ImlHL8OqiSFe3FTpuOGRzkq/GW4Z9Ojag1AICInFdbCyso7VZ4OwBkc+H3pbVWp2O2pLO7+2MAHtvKPoQQ/UF/QSdEIijYhUgEBbsQiaBgFyIRFOxCJMKWVuPfKxkApRyRjSJ/XHc9kdgOzfCEkOmpCWorxaSVSFZTvRFOGFlrcVnII/srlCIJNJFEGO/y441OhBOA2i2+v0Ke+xFJRkS2wN+0RjM8V602n4/ByP5yQ9zHYmRc28LyYCaSRdeOZKjFMi2Hh3jyVXW1Rm2tdlhiiyUcVlYuB7d3o9mjQogkULALkQgKdiESQcEuRCIo2IVIhL6uxps5ihZOQCiXuSu3zI4Ht+8p8cyJfJeXWqou8uSUTpff/+q1sO8ZngeDkUiZq1xkFXn5coWPi7xrE+XwinBlhSetNCMJLXWSpAHE66oNk9JOrSZP1Mh0+InlIwk5HVKKCwByZPm80eBjCnn+hma6PIGmUV2iNpAkKgAYIJdxu8sVg8urYUWmE6knqCe7EImgYBciERTsQiSCgl2IRFCwC5EICnYhEqGv0lvODOMD4UOWItLKKEmCmBrhNb86pP0QgEgfEyCbixRCI3XEGt2I9BPRyXKRZIxOg0tUnuX36AsXwl1mOi1+1pUaT9KodbhMOVyKdHdpkPZP4OecMS4bZQcinVhWucw6mA/7mIu0VlqL1A2st7j01o007Vquch+Xa+Hrp0qkXgBYa4WvgWak1qCe7EIkgoJdiERQsAuRCAp2IRJBwS5EIijYhUiELUlvZnYaQAXralbb3Y9GD5Y1TI2FJZRynktexWLYlslyqaMUqe/WanMZqhvJ5FpvQ/+LNCP14jpNLst1PZJRFpG8PMezsirNcAZbp8PntxZpNdWO2Cqr3P9zi2E/8hm+v5Eqn/vWW7w9WP0ylw6vm7wpuH16+gAdY+VwfTcAaCxdorZqlWcPXq5w6e3i5bDMevoM96OTDYduo8nluu3Q2T/s7vydEEJcE+hjvBCJsNVgdwD/aGZPmdmx7XBICLEzbPVj/P3uft7MpgH80MxedvcnrvyF3k3gGAAUI9/LhRA7y5ae7O5+vvfzAoDvA7gn8DvH3f2oux8t5PStQYjd4qqjz8yGzKz89msAvwng+e1yTAixvWzlY/wMgO/32iXlAPxvd/8/sQH5XBb7p8KFCEcKXDIYHgxLTRaRrhDJQLJItlmjzmWcDJHl9pR5G6qhIZ6ttXKZixijIzyjrBIpAvnGufA+qw3+FarApwOzg5GsvTzPzDt9KZx91/BIkdBI1tvoSJna7rudK74rc2GZ1WuRY03ybMpGjc9HtcqfnQN5vs+De8PnNj09Q8fMr4SlvEuvvEXHXHWwu/spAHde7XghRH/Rl2ghEkHBLkQiKNiFSAQFuxCJoGAXIhH6W3Aya5goh7PRcs2wVAMAA/mwm4MD4b5mANCoc3mqFenXNTYW7isHAE6KFDY7/J7ZakWKIQ7zPnDnF8K9vADgtTd4NtRCJXxukdqFuD7SM+/j//4ItR3Yx/3/26dOBbf/5CSXhtpdnumXy3CprLK8QG21angey2UuhaHDs++KRT6uQLIzAWDQ+Lh2J/zmXHdwPx1TXgz3Anz2dT4XerILkQgKdiESQcEuRCIo2IVIBAW7EInQ39X4XA7TE3uCtvoiX7XOWNjNKmmbAwD1WC0ui9Rji7RJYnfGeouvIo+N84SWZoevMJ86e57aFle4j6w+XTbSMmqkyPc3nQuv+gJAcZErBjeP7A1un5vgfswvX6C2Ro3P8dOvvEJtGdIOqTUUaV01yhNQkOEhMzrK1aFyN9JuitQp9OYKHXOIJJQN5Pn86skuRCIo2IVIBAW7EImgYBciERTsQiSCgl2IROiz9JbH+ORU0DY+zNs1ZTLhJILllSU6prVa5fvrxNo/8YJsThJyhod5nbkWuO2lU1wyWm3wVkLF4gC3FcI+loa4LDSe5TLlUyfnqa3d5JdPYzQsvU2N8/kwcDms1ebSbK3Ja+GtklpzzTY/Z4tIqZHuYMhnIq3DMpHae7nwPLYbXNp0ItuSXC0AerILkQwKdiESQcEuRCIo2IVIBAW7EImgYBciETaU3szsYQC/DeCCu7+/t20CwLcBHAJwGsCn3J3rYP+2N4DIaBZpj8MYiNQDG0Q4KwgAcpF7XCYTqSdHZLmBEm//dPEtnjVWu8in7MYJLlE1uAqFIpHYbj08S8dkIjtsZ/kcr0Skz1w2XCevXODvy57xw9R2+ObrqO31N5+ktpdfORfcXshFZC3nsm27zUMmQzIOASBf4PPY7Yavq25E5zMLX6cRZXBTT/a/AvDAu7Y9BOBxd78ZwOO9/wshrmE2DPZev/XFd23+GIBHeq8fAfDxbfZLCLHNXO139hl3nwOA3s/p7XNJCLET7PgCnZkdM7MTZnaiUot82RRC7ChXG+zzZrYPAHo/aT0hdz/u7kfd/Wh5kC86CSF2lqsN9kcBPNh7/SCAH2yPO0KInWIz0tu3AHwIwKSZnQXwBQBfAvAdM/sMgDcBfHIzB+u6o74WLq5nLZ65BIQzlFZXeUG+Zovfx9oZ/gmjWuNS2QqxzR7k0+htvr/rJ7lQcng/l2pqa3zc7C13BrcXnH+FWrrMC3eWxsIFQgEAl3gm18G9+4Lbl1d5Nt+N/+5mahsZ51l7I+O3UdvSQnj+ly7zFlr5iDyYcZ5x2OpGsil5MiU6rfD1HUmio63IIklvGwe7u3+amD6y0VghxLWD/oJOiERQsAuRCAp2IRJBwS5EIijYhUiEvhacdDg6FpYnvMMLADKZoVTkRSqHy1yqOb/AZb7Xzy5QWy4f9qMwz/uyrc3z/d08zeW1j3yIy1CvnXt3qsK/UZ4NF/Sc3BMuAAkAFxZ4UcmxsYgM1eX+F0iBxQsL4Sw0AMgVl6ltYXmO2s7N8Sy1fD58HYyNcC2sXucCluf489EiWlk3IstlLDzOIhmYkTaB/DjvfYgQ4pcRBbsQiaBgFyIRFOxCJIKCXYhEULALkQh9ld6y2QzGxoaDtnaOS2/Vajhjy1tczrhc4VlNb7zJpaZqlcs4pWL43jj3Os++mynyIoSzs9dT29j+G6gtX4mkUJEinAfuvIcPeYvLYaU2lw474Jl0q6th277BsDQIAM0OPy8bCl83AHBgaD+1lcfCkmPl0lt0zIX5S9TWMi43rjV5EUtkuFY2NBDOwmzWI5IiKWBpRMYD9GQXIhkU7EIkgoJdiERQsAuRCAp2IRKhr6vx3U4bleXwSmeuyWu15UmrG/ASaMhlubFW5Sv142We+DE2FF41rS/x1fjp/byG2+wd/4Hanj/bpLZXTnLbffsmgtuXl/mYmcPhunUAkEGN2poNvlI/5uGV9ZULfKW71OS18PZNhM8LAJY7vC5c/o7x4PZ6JLHmXx57lNrOnuHnnI20eIo1ZmJ5N61Ym7JWeK5Y0higJ7sQyaBgFyIRFOxCJIKCXYhEULALkQgKdiESYTPtnx4G8NsALrj7+3vbvgjg9wG8rUN83t0f28wBs0SB6ET+6N+JbJEhbaEAoGNcelviCg9WViL1xxph+WrfKJfrfu3DH6a2A7feS23f+8uHqW1vJCkk2wzX1zt36jW+vxtvp7binpuobci5XFpbDPf6LHXDUhgANOtc5rtY4baxKZ40tGfvoeD2enWEjslwEzoFnvwTq0HXanHp09rhhC5znujVbodDd6vS218BeCCw/S/c/Ujv36YCXQixe2wY7O7+BABezlQI8UvBVr6zf9bMnjWzh82MfzYTQlwTXG2wfw3AYQBHAMwB+DL7RTM7ZmYnzOxEtca/twghdparCnZ3n3f3jrt3AXwdAC2D4u7H3f2oux8dHuRVW4QQO8tVBbuZ7bviv58A8Pz2uCOE2Ck2I719C8CHAEya2VkAXwDwITM7AsABnAbwB5s5mAEwogx0SBYPwNvgRDrxwOuR/UVKuE3s4W2j9g6Gpb67j95Cx9x2H5fXli5wuXGgzTPzbjxwgNq65OT2TvPab+01LmHWItlyzTYf16qHL60OuGz42rmz1Pbc8yeo7b57uY979oazDlcqYWkQAEjHKADA5CEus3Zj7ZqaERmNSLqXF3g7rEYl7GSXZBsCmwh2d/90YPM3NhonhLi20F/QCZEICnYhEkHBLkQiKNiFSAQFuxCJ0NeCk+5Al2T41BtcMiiQLK9cjhf4y2a4HHPTXv7XvcUSv/8duv5gcPudv84z2/bdege1PfOTv6S26w5yH/e+7wPUVpg6HNyeGxylY2prXAKsr/DMtvnzZ6htaT4so3VaPHutVA4X9ASAyUn+Xp85/zS1zeybDW5v1yJZlnXexslWl6it4+GMQwBwpjkDKA2Ez62wl5/zygDJBI1EtJ7sQiSCgl2IRFCwC5EICnYhEkHBLkQiKNiFSIS+Sm9mhnw2fMilSEHBzlpYZigNluiYbIZLHdORzLYzczzT6PDdoVJ8wIEPhLevwyW0VmWV2kbLXCqbuuUIta3mwj3RXnj6STqmUed+rKzw+bh47k1qy3bC0mexyC+52RvCMhkA3HELL3zZzvJMtHx2LLy9wLMic2u8qGTtjXPUxmRlAGhHHqtV0pdwcA8/rxnSQzCfj/SH4y4IIX6VULALkQgKdiESQcEuRCIo2IVIhP4mwnS7aNTDK52DA9wVK4ZXK/MZXgPNO9xWGuatoX7nv/wOtd33Wx8Jbh+ZnKFj5k+9RG3ZiP/LFV6DbuH0v1Lb+Up4RfhHf/d3dMxwiSdcrDV4wsjeGa4YjJTDK8mvn+XJM83IfEzsP0Rtt3zgg9SGzkBw8+Iyr3dXI+oPACzVuY/m/Bpeq/NErypp2eRVrgrcFhYZ0OUilJ7sQqSCgl2IRFCwC5EICnYhEkHBLkQiKNiFSITNtH86COCvAewF0AVw3N2/amYTAL4N4BDWW0B9yt15gS4ADkfXSW24Lk8isHZYtmh7pMVTpOZXcWCE2o58kMs4A/mwRPXiM7wG2tL516it0eDSSmVpkdrOnHyR2qoeTg7Kd/ixhnNcihwp8mSMqXEuvc3NvxXc3o60+apVuMx35nWedAO8QC3VariGXjHHr4/2wDS1XWrza6dU4jX0Bss8aauUC8uDldoKHdPuhiXAiPK2qSd7G8Afu/ttAO4F8IdmdjuAhwA87u43A3i8938hxDXKhsHu7nPu/rPe6wqAlwDMAvgYgEd6v/YIgI/vlJNCiK3znr6zm9khAHcB+CmAGXefA9ZvCAD4Zx8hxK6z6WA3s2EA3wXwOXfnXyZ+cdwxMzthZidW67yWuxBiZ9lUsJtZHuuB/k13/15v87yZ7evZ9wEINrx29+PuftTdjw6VCtvhsxDiKtgw2M3MsN6P/SV3/8oVpkcBPNh7/SCAH2y/e0KI7WIzWW/3A/g9AM+Z2TO9bZ8H8CUA3zGzzwB4E8AnN96VY129+0W6bf4RP5cP14zrRGp+NcGzk2ZGeV24f3j076ltYiYs8UzvC7eFAoBmjWev5fNhyQUAhoe4xJPLcKlsiMiDe6fDNcsAoF7himkpy328tHCR2lrN8HtTLnIJqlnl0turT5+gtrmXX6G2Rpu0ZMrzOezE5vcAlyIxxK/hzACXPotERhsHn6vb3ndDcHupeIqO2TDY3f2fAbCcv3DOpxDimkN/QSdEIijYhUgEBbsQiaBgFyIRFOxCJEJfC07CDd1ueGG/EMm8KuZIsb4MLwzokZZA3SbPvLp4MZytBQDVhbCt1OJ/UNgFP6+JcS6Hje2forZ2p0Ft586HffRIPlQmwy+DZptLmFnjhSqHimG5lCQwru8vZoxkMXaaXN7MkOttpcblxuYAkesAlPfzuV8t8VZZlS6X5dZWw8/cPSM30jGTRErN5fl7qSe7EImgYBciERTsQiSCgl2IRFCwC5EICnYhEqG/0hsMGQtnURUHeIaPkwy2oVJY3gGAofIktdVaPANpT5nn3OeIH83L83RMN8P3V8tzqWlmJpzVBADdJpdxbr3jQHD7j//pcTqm6TVqyxuXN+tVPm6kHM7aK+T4JZe1SD+0Nf6evT7HZbTl5fB71rBVOmbqFv4MnB2LZO05f6+XLvK5KqyFJcyh2UimYi2cVdiNqJd6sguRCAp2IRJBwS5EIijYhUgEBbsQidDX1fiMAYVc+P5Sa/AEgyxpQdSN1EertXgyQzbPkyoGCny1NZ8P+1EY5G2QRkd4Qs5bC3wVvzYbXlUHgOmDN1HbuQvhunDv+7X76ZjqwnlqO/UKb620WuWJH7lseP5HR3ltPSP1CQFg7hz38c03IokwA+H5H5nhSs7URMTHiCpgi/y9Hl/ioTY7PRHcfmCMXwMnXwwnPDXqPMlLT3YhEkHBLkQiKNiFSAQFuxCJoGAXIhEU7EIkwobSm5kdBPDXAPZivXfTcXf/qpl9EcDvA1jo/ern3f2x6MFyhpmp8P2ldekSHVfvhCWZVZ7LAM/w1lC5SDLGyAhPPiiQ1kr1VV6DrhSpCYYmt5348Y+p7cZbuWR39mxYkslE6vUNDvBactmIvFkqcalptRqW3up1Lom2Iy3Ahkvcj/vuuoXaiiQhp53ltfU6LZ60Uj/DpbdMpUht04NlarvrlveFx4zN0DFPzb0e3N5u8fPajM7eBvDH7v4zMysDeMrMftiz/YW7//km9iGE2GU20+ttDsBc73XFzF4CMLvTjgkhtpf39J3dzA4BuAvAT3ubPmtmz5rZw2bGW6MKIXadTQe7mQ0D+C6Az7n7CoCvATgM4AjWn/xfJuOOmdkJMzuxUuPfyYQQO8umgt3M8lgP9G+6+/cAwN3n3b3j7l0AXwdwT2isux9396PufnRkkFfyEELsLBsGu5kZgG8AeMndv3LF9n1X/NonADy//e4JIbaLzazG3w/g9wA8Z2bP9LZ9HsCnzewIAAdwGsAfbLSjQsFw3cHw033UuGxx8kxYCplf4NlrzQ6XaoaH+Wmv1ngGVadbDW7PRu6ZiwtcUqxUuUyy1uJ+ZJ3bysPhpZP5txbpmLOrXE7qOpfsZqa4TGndcPbV0jKvFzcwxN+zsVEuXRWyfP4bTSLB5rjcuNrg+2tWIy2vunzcTQf3Utv+veF5PHOWS6yXFsIx0Y600NrMavw/Awi941FNXQhxbaG/oBMiERTsQiSCgl2IRFCwC5EICnYhEqGvBSezOcPIOMkcI1ICAIxPZ8OGIV408OI8L2C5FmmflCvwYoNsWLfFM+xaHe7H5TqXoYYiWV5rNS6V1dfCBSebER87EZs7mXsA1ZVI+6eRcOHOkRFenLNe5/u7eInP1fAwz76zTPh5Zm0u2xZyvOjoAFeIUSjwuTp00yFqq9fCvjzxxIt0zLOvXAjva43LuXqyC5EICnYhEkHBLkQiKNiFSAQFuxCJoGAXIhH6Kr2ZGXLF8CGLIzzXfWI4fE/K1bmslS/x7J+VSN8tdPj9r1ScDg/J82N1GrwfWmGQ+5HP8fnIZrnk2PCwL80Wlxs9ktlmXKGCN7kE2CGmfCTbDAUuNy4vcemt3uT9zUbHwlJqjkhyAJCJzH0NXNqav1ihtqVIhmNlNZzF+H9/9DI/FlEp15qS3oRIHgW7EImgYBciERTsQiSCgl2IRFCwC5EIfZXeul1DlRXsyw7TccNDYR0nX+K60FAkPWl0lEtl1RXei6y6Ei4AWK1Fst7WuK1c4AUbi6SvHAC0G1xyzOXC9+9C5LaeH+DZWmZ84GCkcGeGmNodLg0VSpEefGNcblxc5JJXhUiRIxN87muRnnOvnuYFRF9+7gy1zUzwbMqZA+TcMvw6nSQFOOcrXIbUk12IRFCwC5EICnYhEkHBLkQiKNiFSIQNV+PNrAjgCQADvd//W3f/gplNAPg2gENYb//0KXfn2QpYr+F29o2wrbHMV8/LU+EV3GIpkgDBF/cxMcFPu7rK66AtL4dtS5d44sQSX7xFtstXwbvOlYZOh6/woxu2xe7qluGJMNkcn6t6JGnIyaJ7nrSFAoB2jbeo6kTq03UiyTXL1fA41hUKABYjiszpk/wNXb60Sm3NVX7AvaPh1lC3XT9LxzAXX31rhY7ZzJO9AeA/uvudWG/P/ICZ3QvgIQCPu/vNAB7v/V8IcY2yYbD7Om93NMz3/jmAjwF4pLf9EQAf3xEPhRDbwmb7s2d7HVwvAPihu/8UwIy7zwFA72c42VsIcU2wqWB39467HwFwAMA9Zvb+zR7AzI6Z2QkzO3G5yosdCCF2lve0Gu/uywB+BOABAPNmtg8Aej+DVevd/bi7H3X3o6PDkQr7QogdZcNgN7MpMxvrvS4B+E8AXgbwKIAHe7/2IIAf7JSTQoits5lEmH0AHjGzLNZvDt9x9783s58A+I6ZfQbAmwA+udGO3HLo5CeDtlbhKB3X6IYTPzLtcKsjACiOcjlpbIp/whjP8ESNiVo4MWF5kbcLWr7I5bX6Kp/+TpvLeXB+j+62wz6u1flXqEIhUu8ux/2vrPFEjTr5ypZ3nmRSzoSTOwCgm+GSUqvF53FgKCxhFvO83t1Ygft4I8ao7QN38jZUt95xJ7Uduumm4PZ77uVy49nz1eD2f3mNx8SGwe7uzwK4K7D9EoCPbDReCHFtoL+gEyIRFOxCJIKCXYhEULALkQgKdiESwTySXbXtBzNbAPB23tskAK4T9A/58U7kxzv5ZfPjenefChn6GuzvOLDZCXfn4rr8kB/yY1v90Md4IRJBwS5EIuxmsB/fxWNfifx4J/LjnfzK+LFr39mFEP1FH+OFSIRdCXYze8DM/tXMTprZrtWuM7PTZvacmT1jZif6eNyHzeyCmT1/xbYJM/uhmb3a+zm+S3580czO9ebkGTP7aB/8OGhm/2RmL5nZC2b2R73tfZ2TiB99nRMzK5rZ/zOzn/f8+O+97VubD3fv6z8AWQCvAbgRQAHAzwHc3m8/er6cBjC5C8f9DQB3A3j+im1/BuCh3uuHAPzpLvnxRQB/0uf52Afg7t7rMoBXANze7zmJ+NHXOQFgAIZ7r/MAfgrg3q3Ox2482e8BcNLdT7l7E8DfYL14ZTK4+xMA3l03ue8FPIkffcfd59z9Z73XFQAvAZhFn+ck4kdf8XW2vcjrbgT7LIAr212exS5MaA8H8I9m9pSZHdslH97mWirg+Vkze7b3MX/Hv05ciZkdwnr9hF0tavouP4A+z8lOFHndjWAPlZDZLUngfne/G8BvAfhDM/uNXfLjWuJrAA5jvUfAHIAv9+vAZjYM4LsAPufuvDRN//3o+5z4Foq8MnYj2M8COHjF/w8AOL8LfsDdz/d+XgDwfax/xdgtNlXAc6dx9/nehdYF8HX0aU7MLI/1APumu3+vt7nvcxLyY7fmpHfs91zklbEbwf4kgJvN7AYzKwD4XawXr+wrZjZkZuW3XwP4TQDPx0ftKNdEAc+3L6Yen0Af5sTMDMA3ALzk7l+5wtTXOWF+9HtOdqzIa79WGN+12vhRrK90vgbgv+6SDzdiXQn4OYAX+ukHgG9h/eNgC+ufdD4DYA/W22i92vs5sUt+/C8AzwF4tndx7euDH7+O9a9yzwJ4pvfvo/2ek4gffZ0TAHcAeLp3vOcB/Lfe9i3Nh/6CTohE0F/QCZEICnYhEkHBLkQiKNiFSAQFuxCJoGAXIhEU7EIkgoJdiET4/41iX1zpog9jAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6], dtype=uint8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = len(set(list(y_train.reshape(y_train.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = Input(shape=(X_train[0].shape))\n",
    "x = Conv2D(32,(3,3),strides=2,activation='relu')(i)\n",
    "x = Conv2D(64,(3,3),strides=2,activation='relu')(x)\n",
    "x = Conv2D(128,(3,3),strides=2,activation='relu')(x)\n",
    "x = Conv2D(256,(3,3),strides=2,activation='relu')(x)\n",
    "\n",
    "x = GlobalMaxPool2D()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(1024,activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(k, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(i,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_thresh = 0.96\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        if(logs.get('accuracy') > acc_thresh):\n",
    "            print(\"Stopping training as desired accuracy reached....\")\n",
    "            self.model.stop_training\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 0.4368 - accuracy: 0.8441 - val_loss: 1.1016 - val_accuracy: 0.6721\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.3985 - accuracy: 0.8577 - val_loss: 1.1973 - val_accuracy: 0.6644\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3828 - accuracy: 0.8639 - val_loss: 1.2417 - val_accuracy: 0.6682\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.3583 - accuracy: 0.8720 - val_loss: 1.2601 - val_accuracy: 0.6738\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3421 - accuracy: 0.8794 - val_loss: 1.2669 - val_accuracy: 0.6663\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 0.3189 - accuracy: 0.8884 - val_loss: 1.3130 - val_accuracy: 0.6675\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.3102 - accuracy: 0.8910 - val_loss: 1.3994 - val_accuracy: 0.6642\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.2993 - accuracy: 0.8949 - val_loss: 1.3850 - val_accuracy: 0.6650\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.2826 - accuracy: 0.9025 - val_loss: 1.4496 - val_accuracy: 0.6689\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.2765 - accuracy: 0.9037 - val_loss: 1.4600 - val_accuracy: 0.6674\n"
     ]
    }
   ],
   "source": [
    "r = model.fit(X_train,y_train , epochs =10 , validation_data=(X_test,y_test) , callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 15, 15, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 1, 1, 256)         295168    \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 661,834\n",
      "Trainable params: 661,834\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[[-4.68172804e-02, -1.31001115e-01,  1.34572029e-01,\n",
       "            1.43149257e-01,  2.30107933e-01, -2.74789780e-01,\n",
       "           -1.02655649e-01, -2.08083674e-01,  1.33426249e-01,\n",
       "           -1.89854324e-01,  4.71040495e-02, -2.57009417e-01,\n",
       "           -1.93473667e-01, -1.13071762e-01, -2.45083421e-01,\n",
       "            1.08689100e-01, -2.24207968e-01,  3.20641138e-02,\n",
       "           -2.47576922e-01, -7.39834011e-02,  2.90583428e-02,\n",
       "           -1.17114680e-02,  2.78848469e-01, -9.16115288e-03,\n",
       "            1.27170682e-01, -1.72494754e-01,  1.39009193e-01,\n",
       "            5.54054864e-02, -1.65055081e-01, -1.47544026e-01,\n",
       "           -5.33591136e-02, -2.40255576e-02],\n",
       "          [-5.76829910e-02, -7.33849704e-02,  1.60439149e-01,\n",
       "           -1.59724080e-03,  1.07555546e-01, -4.80823740e-02,\n",
       "            2.67113969e-02,  2.50074174e-02, -8.70639011e-02,\n",
       "            1.37481140e-02,  1.79870665e-01, -3.63153189e-01,\n",
       "           -1.21897377e-01, -1.14224389e-01,  1.07213274e-01,\n",
       "            3.46163847e-02, -2.56374031e-01,  4.67085503e-02,\n",
       "           -2.21492410e-01,  1.23521760e-01, -1.55691385e-01,\n",
       "           -1.41512156e-01,  2.61544134e-04,  7.37247840e-02,\n",
       "           -2.45123003e-02, -3.29547256e-01,  1.53949782e-01,\n",
       "           -1.46564588e-01,  2.36890670e-02, -4.05945271e-01,\n",
       "           -9.20257065e-03, -5.92065677e-02],\n",
       "          [-2.39146307e-01,  5.91452345e-02,  1.72670096e-01,\n",
       "            1.08737931e-01,  1.04442902e-01, -2.74380576e-02,\n",
       "            9.26309675e-02, -4.80855405e-02,  3.77592474e-01,\n",
       "            2.18253974e-02, -2.51564234e-02, -2.73799837e-01,\n",
       "           -2.01728567e-01, -6.09228015e-02, -2.41106793e-01,\n",
       "           -1.64060891e-01,  4.55090441e-02,  1.78235903e-01,\n",
       "           -1.29330158e-01, -1.37790918e-01, -3.69851232e-01,\n",
       "            1.05415538e-01, -1.44409046e-01,  7.73315430e-02,\n",
       "            1.68023318e-01, -3.43091279e-01,  1.41549096e-01,\n",
       "            1.47151132e-03,  1.74206570e-01,  1.62948385e-01,\n",
       "           -9.27556232e-02, -1.37691990e-01]],\n",
       " \n",
       "         [[ 1.18827201e-01, -8.95706788e-02,  3.50979306e-02,\n",
       "            2.09832788e-02,  3.84493582e-02, -3.57386857e-01,\n",
       "            1.42701134e-01,  2.74036974e-01,  1.57308206e-01,\n",
       "            1.81085527e-01,  2.24539861e-01, -2.73563825e-02,\n",
       "           -2.35672534e-01,  2.33838838e-02, -1.24997810e-01,\n",
       "            7.82557763e-03,  1.31475359e-01, -2.36930937e-01,\n",
       "           -1.87079951e-01, -1.74815729e-01, -2.36083776e-01,\n",
       "            2.67093275e-02,  1.97633162e-01, -6.80107772e-02,\n",
       "           -1.59913469e-02,  5.71655184e-02, -1.29400268e-01,\n",
       "            1.90430600e-02, -9.35845375e-02, -1.90483361e-01,\n",
       "            2.70696014e-01, -1.59698009e-01],\n",
       "          [ 1.87935770e-01, -6.00796416e-02,  1.80243328e-01,\n",
       "           -2.59176701e-01, -1.34116933e-01, -2.05433816e-01,\n",
       "            1.00797720e-01,  1.91189393e-01,  9.89388898e-02,\n",
       "            2.65382916e-01,  2.01389372e-01, -2.82072932e-01,\n",
       "           -1.26918390e-01,  1.14647700e-02,  2.14428291e-01,\n",
       "           -1.15454026e-01,  5.65473512e-02,  1.26102045e-01,\n",
       "           -2.06412867e-01,  8.64613503e-02, -3.51004243e-01,\n",
       "            1.36052936e-01,  3.39762643e-02, -1.57352209e-01,\n",
       "           -4.00059484e-02,  1.86759308e-01, -2.99584083e-02,\n",
       "           -3.20071995e-01,  7.63978884e-02, -2.87073851e-01,\n",
       "            2.74479896e-01, -2.25510851e-01],\n",
       "          [-1.66056782e-01, -5.11235632e-02,  2.52191573e-01,\n",
       "           -1.69378713e-01,  3.08071002e-02, -3.46374661e-01,\n",
       "            2.41053089e-01,  2.63977051e-01,  9.82719585e-02,\n",
       "            1.96620628e-01,  2.08487898e-01, -6.62343949e-02,\n",
       "           -1.55091092e-01, -6.04511499e-02,  1.20476456e-02,\n",
       "            1.44103259e-01,  2.40899518e-01,  2.69121647e-01,\n",
       "           -1.40149340e-01, -2.15901971e-01, -5.46688139e-01,\n",
       "            9.61929485e-02, -1.00997709e-01, -1.00333631e-01,\n",
       "           -4.54685010e-04, -4.89998758e-02, -8.94175023e-02,\n",
       "            1.55115321e-01,  2.08737627e-01,  1.85760707e-01,\n",
       "           -6.95759580e-02,  1.98896173e-02]],\n",
       " \n",
       "         [[ 1.75768763e-01, -1.46431148e-01,  9.20416936e-02,\n",
       "            1.38168633e-01, -2.20498204e-01, -2.67128915e-01,\n",
       "            1.89552709e-01,  3.95767726e-02,  1.32634398e-02,\n",
       "            1.32139802e-01, -1.06652575e-02,  1.75351024e-01,\n",
       "            1.15997596e-02, -4.08334471e-02, -8.36386010e-02,\n",
       "            1.73566505e-01,  1.17976412e-01, -2.37343609e-01,\n",
       "            2.66328365e-01,  1.53957671e-02, -1.01837717e-01,\n",
       "           -1.78321376e-01,  2.06411168e-01,  3.58553231e-02,\n",
       "           -1.03778906e-01, -1.76752433e-01,  1.38008028e-01,\n",
       "            1.25114575e-01, -2.63586879e-01,  6.72644749e-02,\n",
       "            5.93406744e-02, -1.97356835e-01],\n",
       "          [ 7.95691162e-02,  4.50391769e-02,  1.06921948e-01,\n",
       "           -1.47434875e-01, -1.88965768e-01, -2.18561336e-01,\n",
       "           -1.23173051e-01, -2.59188712e-01, -8.73269960e-02,\n",
       "            8.29003304e-02, -1.05096281e-01,  8.42439011e-02,\n",
       "           -1.69898942e-02,  6.75107092e-02,  2.39371017e-01,\n",
       "            1.51777873e-03,  1.77299246e-01, -1.08674593e-01,\n",
       "            5.61554357e-02,  4.94972989e-02, -1.97736427e-01,\n",
       "            1.02986962e-01, -2.47218266e-01,  5.98410927e-02,\n",
       "           -1.79257631e-01,  1.37787253e-01,  2.78322071e-01,\n",
       "           -2.22739920e-01, -1.70534685e-01, -3.71051431e-01,\n",
       "            2.47341599e-02, -1.35787427e-01],\n",
       "          [-2.05165431e-01, -1.37228712e-01,  4.05368805e-02,\n",
       "           -1.52964704e-02, -1.88187122e-01, -1.96884587e-01,\n",
       "            2.18956172e-02, -2.92592525e-01,  2.48664960e-01,\n",
       "            1.77337721e-01,  3.11817210e-02,  2.20484897e-01,\n",
       "            3.35691050e-02,  9.15322527e-02, -1.48751900e-01,\n",
       "            5.91638386e-02,  1.65379763e-01,  1.56739771e-01,\n",
       "            2.16617376e-01,  1.43130003e-02, -1.09249569e-01,\n",
       "            1.29430309e-01, -1.95614934e-01, -1.62670168e-03,\n",
       "           -1.01340182e-01,  8.30613002e-02,  2.05245644e-01,\n",
       "            2.01918289e-01,  7.28573352e-02, -8.11851770e-03,\n",
       "           -1.63240641e-01, -2.89410233e-01]]],\n",
       " \n",
       " \n",
       "        [[[-3.77666056e-02, -1.51325036e-02, -1.72966644e-01,\n",
       "            9.17721316e-02,  3.46678644e-01,  1.25359297e-01,\n",
       "            2.31106013e-01, -2.68696338e-01, -1.39447927e-01,\n",
       "           -3.82816106e-01,  2.46907189e-01,  8.64189565e-02,\n",
       "            1.10943444e-01,  5.22426143e-02, -1.71091035e-02,\n",
       "            9.94328633e-02, -1.34604707e-01, -2.20057219e-01,\n",
       "           -3.10804307e-01,  1.26359448e-01, -1.84578165e-01,\n",
       "            3.51175778e-02,  2.45180219e-01,  6.88737258e-02,\n",
       "            1.68715209e-01,  2.93375868e-02, -8.52387473e-02,\n",
       "            2.26200432e-01, -4.34813043e-03, -1.21153593e-01,\n",
       "           -2.54670978e-01, -1.73299775e-01],\n",
       "          [-1.66714028e-01, -1.28019705e-01, -2.67295390e-02,\n",
       "           -3.36440699e-03,  3.74298573e-01,  1.65442169e-01,\n",
       "            7.76996166e-02, -2.15365380e-01, -4.24918503e-01,\n",
       "           -1.88383505e-01,  2.89796889e-01, -8.74928236e-02,\n",
       "            7.69915059e-02, -9.76608992e-02,  2.79504329e-01,\n",
       "            4.86419834e-02, -1.20297879e-01, -2.90879346e-02,\n",
       "           -3.45186323e-01,  1.20955268e-02, -3.63161296e-01,\n",
       "           -8.04555267e-02, -6.65927455e-02, -1.21249810e-01,\n",
       "            1.99134797e-01, -6.99991658e-02, -1.01461858e-01,\n",
       "           -2.17601478e-01, -3.34179141e-02, -1.88039750e-01,\n",
       "           -1.89993791e-02, -1.97786570e-01],\n",
       "          [-1.92677274e-01, -9.44247991e-02, -4.66095321e-02,\n",
       "           -7.07030818e-02,  3.81107152e-01,  1.11273653e-03,\n",
       "            1.33693010e-01, -8.09844434e-02, -7.52050504e-02,\n",
       "           -1.58720002e-01,  2.95319021e-01,  3.07434518e-02,\n",
       "            7.41905496e-02, -4.72069122e-02, -2.85785999e-02,\n",
       "           -1.13453716e-01, -1.18599214e-01, -2.56446470e-02,\n",
       "           -3.91680211e-01, -1.61244839e-01, -2.30454624e-01,\n",
       "            7.53358603e-02,  1.59375086e-01, -4.85282056e-02,\n",
       "            2.54034728e-01, -1.56411842e-01, -3.19128763e-03,\n",
       "            4.58395779e-02,  4.48200069e-02,  1.40514582e-01,\n",
       "           -4.59141642e-01, -4.38614152e-02]],\n",
       " \n",
       "         [[ 1.41517922e-01, -1.06828593e-01,  2.04816028e-01,\n",
       "           -1.18090294e-01, -3.06789614e-02,  6.63575307e-02,\n",
       "            8.99208784e-02,  3.73529822e-01,  3.20430771e-02,\n",
       "           -1.74098104e-01, -1.49328813e-01,  1.06756173e-01,\n",
       "            1.51298717e-01, -9.39524323e-02,  3.47384177e-02,\n",
       "            2.85708636e-01, -2.42089763e-01, -3.13575983e-01,\n",
       "            2.52715833e-02,  1.17267236e-01, -1.06212072e-01,\n",
       "            3.72231416e-02,  3.56762677e-01, -7.18911812e-02,\n",
       "            1.48786426e-01,  2.56255478e-01,  3.75977419e-02,\n",
       "            1.96843967e-01, -2.39309445e-01, -2.02674165e-01,\n",
       "            1.55967504e-01,  1.53660178e-01],\n",
       "          [ 1.79683551e-01,  4.49116454e-02,  2.28956938e-01,\n",
       "           -2.44672433e-01,  8.40346739e-02,  1.73613921e-01,\n",
       "            5.19863255e-02,  3.09437364e-01, -3.66355598e-01,\n",
       "            1.37708962e-01, -6.62785918e-02,  3.31392996e-02,\n",
       "            2.28485286e-01, -1.64967522e-01,  3.10465485e-01,\n",
       "            5.95869459e-02, -3.10580909e-01, -3.05641182e-02,\n",
       "            9.78947580e-02,  7.67309815e-02, -2.06216633e-01,\n",
       "            1.10017970e-01,  9.31995288e-02,  7.89504200e-02,\n",
       "            1.70794323e-01,  3.29391330e-01, -1.58726946e-01,\n",
       "           -2.25936219e-01,  4.31682095e-02, -2.59880781e-01,\n",
       "            1.89100146e-01,  8.11769441e-02],\n",
       "          [ 5.27176410e-02, -6.53349832e-02,  2.43367553e-01,\n",
       "           -1.81784779e-01, -1.57310337e-01, -5.81767485e-02,\n",
       "            2.11103722e-01,  3.89545172e-01, -2.28751004e-01,\n",
       "           -1.16352536e-01, -1.65879264e-01,  1.52273938e-01,\n",
       "            2.43222192e-01,  1.06076449e-01, -1.81843773e-01,\n",
       "            1.16193727e-01, -3.95983737e-03,  1.32038012e-01,\n",
       "            6.36502355e-02, -1.88868791e-01, -1.97137102e-01,\n",
       "            2.48393506e-01, -5.10399565e-02,  9.51591972e-03,\n",
       "            1.07829802e-01,  3.25531572e-01, -1.64993778e-01,\n",
       "            1.14660030e-02,  2.15682983e-01,  1.44631907e-01,\n",
       "            1.30756926e-02,  2.58541882e-01]],\n",
       " \n",
       "         [[ 1.90977678e-01, -1.77810360e-02,  1.54408157e-01,\n",
       "            6.40421957e-02, -3.39112878e-01,  1.13914296e-01,\n",
       "           -5.84577210e-02, -1.47384435e-01, -1.92991659e-01,\n",
       "           -8.67632125e-03, -2.97798455e-01,  1.99439302e-01,\n",
       "            1.74114168e-01, -1.64060503e-01, -2.12519407e-01,\n",
       "            2.09546462e-01,  1.38060346e-01, -2.37571210e-01,\n",
       "            3.47912461e-01, -5.83377443e-02,  3.18281204e-02,\n",
       "           -1.47651881e-01,  2.53271133e-01, -9.36859753e-03,\n",
       "            1.99766047e-02, -1.75931975e-01,  9.55098346e-02,\n",
       "            2.38054186e-01, -4.01078671e-01, -1.24910221e-01,\n",
       "            1.20057344e-01,  1.40318245e-01],\n",
       "          [-2.58252453e-02, -5.33130988e-02,  1.31620780e-01,\n",
       "            1.33165643e-01, -3.34671170e-01, -1.49629503e-01,\n",
       "           -3.21768939e-01, -9.30855870e-02, -4.83540326e-01,\n",
       "            7.73292631e-02, -1.64769799e-01, -1.55586883e-01,\n",
       "            2.41705209e-01, -8.58547539e-02,  2.52632678e-01,\n",
       "           -3.14585329e-03,  1.68178633e-01, -1.06286898e-01,\n",
       "            1.61683738e-01,  1.27428339e-03, -2.45826636e-02,\n",
       "            1.49523437e-01, -2.41965085e-01, -1.66278705e-01,\n",
       "            5.20361997e-02,  1.90072451e-02,  2.43647784e-01,\n",
       "           -2.18618214e-01, -3.28774042e-02, -4.00146693e-01,\n",
       "            2.63516873e-01,  1.64825976e-01],\n",
       "          [-2.14123324e-01,  7.06989020e-02, -3.79454643e-02,\n",
       "            1.00288965e-01, -3.21627706e-01,  2.29486432e-02,\n",
       "           -1.68940082e-01, -2.82772750e-01, -2.41494924e-01,\n",
       "            1.63731277e-01, -3.71447980e-01, -6.48318082e-02,\n",
       "            2.00137302e-01, -3.27579305e-02, -5.01329303e-02,\n",
       "            8.57049748e-02,  3.15620750e-01,  7.13111013e-02,\n",
       "            2.31541142e-01, -2.45551795e-01,  5.78864664e-02,\n",
       "            1.37397945e-01, -1.49875715e-01, -6.59111217e-02,\n",
       "           -1.16214678e-01, -1.34692462e-02,  7.27759302e-02,\n",
       "           -7.11662993e-02, -6.45749941e-02,  5.40651232e-02,\n",
       "            1.37109039e-02,  2.23764554e-01]]],\n",
       " \n",
       " \n",
       "        [[[-2.59806603e-01, -1.25779867e-01, -2.86040548e-02,\n",
       "           -5.02613373e-02,  1.37119785e-01,  1.53238565e-01,\n",
       "            1.70661002e-01, -2.59878516e-01,  8.47598985e-02,\n",
       "           -1.35645539e-01, -1.82687268e-01,  1.74475089e-01,\n",
       "           -1.68516710e-01, -1.46403879e-01, -7.85893872e-02,\n",
       "           -5.08027077e-02,  3.55376422e-01, -7.20125139e-02,\n",
       "           -1.03834597e-02, -4.97204391e-03,  6.86317906e-02,\n",
       "           -2.03862205e-01,  1.31978810e-01, -2.82613132e-02,\n",
       "           -1.12665914e-01, -1.05960935e-01,  2.68619470e-02,\n",
       "            1.57634672e-02, -1.29562840e-01, -1.89991649e-02,\n",
       "           -2.24232927e-01, -1.68196261e-01],\n",
       "          [-1.71792969e-01, -7.99167380e-02, -1.38908654e-01,\n",
       "           -2.24882305e-01,  9.96134505e-02,  8.40841159e-02,\n",
       "           -2.40263082e-02, -1.21726230e-01,  1.59827601e-02,\n",
       "            3.99749838e-02, -2.28745759e-01,  5.31544760e-02,\n",
       "           -1.85981188e-02, -1.43909961e-01,  1.98776305e-01,\n",
       "            4.11915546e-03,  6.47834167e-02, -4.12672199e-03,\n",
       "           -2.08932176e-01,  1.17205590e-01,  4.07825783e-02,\n",
       "           -1.54946148e-01, -1.22708082e-01,  3.63433291e-03,\n",
       "           -1.38009712e-01,  4.03198153e-02, -8.58012959e-02,\n",
       "           -5.97475879e-02, -9.16981623e-02, -2.07130834e-01,\n",
       "           -2.34344423e-01, -1.63126141e-01],\n",
       "          [-1.69963375e-01,  5.51614538e-02, -2.12142929e-01,\n",
       "           -1.89082921e-01,  1.91418007e-01,  2.27168441e-01,\n",
       "            4.88653965e-02, -1.31728232e-01, -4.51578107e-03,\n",
       "           -8.27967748e-03, -1.17924601e-01,  1.61073729e-01,\n",
       "            2.66703293e-02, -1.27106681e-01, -1.24434628e-01,\n",
       "            2.72226986e-02, -5.80146313e-02,  1.99324399e-01,\n",
       "           -7.83594698e-02,  3.73097043e-03,  2.16559336e-01,\n",
       "            1.75148308e-01,  3.96322682e-02, -5.94757348e-02,\n",
       "            6.48037866e-02, -9.17606503e-02,  5.64998426e-02,\n",
       "           -3.06633729e-02,  1.03649154e-01,  4.54540066e-02,\n",
       "           -3.33974659e-01, -2.05881700e-01]],\n",
       " \n",
       "         [[-1.87629193e-01, -1.70124158e-01, -3.00237566e-01,\n",
       "            3.94980945e-02, -7.33960420e-02,  1.70821398e-01,\n",
       "           -2.12583944e-01,  1.61308676e-01,  1.04592972e-01,\n",
       "           -2.83104479e-01, -2.20683962e-01,  1.43693656e-01,\n",
       "            1.77074969e-03,  3.16313505e-02, -1.83693439e-01,\n",
       "            1.60949722e-01, -2.09177822e-01,  5.28570674e-02,\n",
       "            1.84483603e-01,  2.16752186e-01,  2.37023413e-01,\n",
       "           -1.09957755e-01,  6.37684837e-02,  6.50271922e-02,\n",
       "           -6.44959360e-02,  1.21959962e-01, -3.17374170e-01,\n",
       "            2.88148314e-01, -3.93226236e-01, -2.14165092e-01,\n",
       "            7.28950417e-03, -2.44469389e-01],\n",
       "          [ 6.19421015e-03,  2.96951667e-03, -2.53427625e-01,\n",
       "           -5.57711050e-02, -8.13907981e-02,  1.89079344e-01,\n",
       "           -2.98295259e-01,  6.64292723e-02, -1.71576794e-02,\n",
       "            3.98044065e-02, -3.25589061e-01,  9.87799559e-03,\n",
       "           -1.01225823e-01,  8.15144107e-02,  2.04087719e-02,\n",
       "           -2.46241000e-02, -1.16792880e-01,  2.16753725e-02,\n",
       "            1.49206907e-01,  2.04585761e-01,  1.39303654e-01,\n",
       "           -1.88477978e-01, -1.24419019e-01, -8.24803859e-02,\n",
       "            2.28187945e-02,  6.71780407e-02, -4.07772422e-01,\n",
       "           -2.70087689e-01,  3.60424928e-02, -1.68167964e-01,\n",
       "           -6.34264350e-02, -1.57119662e-01],\n",
       "          [-2.38009378e-01,  8.59143138e-02, -3.16856116e-01,\n",
       "           -1.57974660e-01,  1.17221147e-01,  1.36173069e-01,\n",
       "           -1.94966123e-01,  2.79063672e-01,  1.95477717e-02,\n",
       "           -3.96886654e-02, -2.15617076e-01,  1.43980220e-01,\n",
       "            2.07525976e-02, -9.57079697e-03, -2.54002839e-01,\n",
       "           -1.52873069e-01, -3.29341888e-01,  1.87223524e-01,\n",
       "            7.02404752e-02, -4.27394547e-02,  3.02932858e-01,\n",
       "            1.10823788e-01, -1.23082824e-01, -3.01526692e-02,\n",
       "            1.61907822e-01,  1.65864155e-01, -2.64735222e-01,\n",
       "           -1.66949064e-01,  1.00700995e-02, -2.49584317e-02,\n",
       "           -1.50861278e-01, -1.68889165e-01]],\n",
       " \n",
       "         [[ 1.54897496e-01,  9.91023611e-03, -2.01542571e-01,\n",
       "            2.07963064e-01, -1.19959578e-01,  1.71421856e-01,\n",
       "           -1.34373546e-01,  2.82292049e-02,  2.38132089e-01,\n",
       "           -7.37264529e-02,  6.14630543e-02, -1.40483677e-01,\n",
       "            2.77708843e-02,  9.46857259e-02, -3.35813202e-02,\n",
       "           -6.71578199e-02, -1.86770722e-01, -1.19854793e-01,\n",
       "            7.75062293e-02,  1.58120021e-01,  1.45128340e-01,\n",
       "           -1.89214781e-01, -8.61446559e-02, -6.32685646e-02,\n",
       "           -1.58323616e-01, -3.49680066e-01,  1.28512353e-01,\n",
       "            1.86790258e-01, -2.86595076e-01, -3.27811271e-01,\n",
       "            1.39486372e-01,  1.09054349e-01],\n",
       "          [-1.20475758e-02, -5.65840714e-02, -2.75200069e-01,\n",
       "            2.73063719e-01,  1.70170255e-02,  1.28911033e-01,\n",
       "           -2.76373178e-01,  1.48705514e-02,  6.18584380e-02,\n",
       "            7.74698630e-02,  1.12064518e-01, -3.24807853e-01,\n",
       "            4.28030379e-02, -5.38632050e-02,  2.41672099e-01,\n",
       "           -6.57770038e-02, -1.13698557e-01,  7.90556967e-02,\n",
       "            2.04155460e-01,  1.04895063e-01,  9.09121111e-02,\n",
       "           -9.98518243e-02, -2.04558492e-01,  2.23767143e-02,\n",
       "           -8.96873251e-02, -1.14857472e-01, -1.19770586e-01,\n",
       "           -1.17387854e-01, -1.14334382e-01, -4.72541451e-01,\n",
       "            5.72184809e-02,  7.87738785e-02],\n",
       "          [-5.32544404e-02, -1.71536565e-01, -2.10905835e-01,\n",
       "            2.02841595e-01, -2.04374298e-01,  1.16494671e-01,\n",
       "           -1.22862399e-01, -1.58915386e-01,  1.73106998e-01,\n",
       "            1.98796280e-02,  2.17346922e-01, -5.63477771e-03,\n",
       "           -1.60453450e-02, -2.19734441e-02, -4.60312702e-02,\n",
       "           -1.00710154e-01, -1.66321352e-01,  1.22067742e-01,\n",
       "            1.67229012e-01, -2.17462763e-01,  1.52138337e-01,\n",
       "            1.06041543e-01, -2.01896966e-01, -4.63870317e-02,\n",
       "            2.19638124e-02, -5.57132922e-02, -1.18103912e-02,\n",
       "           -1.20043382e-01, -8.62005353e-02,  8.98508579e-02,\n",
       "           -1.56152830e-01, -6.81943744e-02]]]], dtype=float32),\n",
       " array([ 0.18988293, -0.04115618,  0.00743624,  0.06197038,  0.0182892 ,\n",
       "         0.01103415, -0.00347508,  0.0016603 ,  0.05087671,  0.02310203,\n",
       "         0.04728356, -0.21248566, -0.13552476, -0.06290657,  0.02288935,\n",
       "        -0.41598296,  0.07146993,  0.12296475,  0.02346373, -0.04931533,\n",
       "         0.10439581, -0.13065529,  0.02826407, -0.12467942, -0.1688966 ,\n",
       "         0.02995098,  0.04156672,  0.02159265,  0.06730168,  0.19673596,\n",
       "        -0.13310495,  0.01603141], dtype=float32)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement using Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = ImageDataGenerator(width_shift_range=0.1 , height_shift_range=0.2 , horizontal_flip= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ImageDataGenerator in module tensorflow.python.keras.preprocessing.image:\n",
      "\n",
      "class ImageDataGenerator(keras_preprocessing.image.image_data_generator.ImageDataGenerator)\n",
      " |  Generate batches of tensor image data with real-time data augmentation.\n",
      " |  \n",
      " |   The data will be looped over (in batches).\n",
      " |  \n",
      " |  Arguments:\n",
      " |      featurewise_center: Boolean.\n",
      " |          Set input mean to 0 over the dataset, feature-wise.\n",
      " |      samplewise_center: Boolean. Set each sample mean to 0.\n",
      " |      featurewise_std_normalization: Boolean.\n",
      " |          Divide inputs by std of the dataset, feature-wise.\n",
      " |      samplewise_std_normalization: Boolean. Divide each input by its std.\n",
      " |      zca_epsilon: epsilon for ZCA whitening. Default is 1e-6.\n",
      " |      zca_whitening: Boolean. Apply ZCA whitening.\n",
      " |      rotation_range: Int. Degree range for random rotations.\n",
      " |      width_shift_range: Float, 1-D array-like or int\n",
      " |          - float: fraction of total width, if < 1, or pixels if >= 1.\n",
      " |          - 1-D array-like: random elements from the array.\n",
      " |          - int: integer number of pixels from interval\n",
      " |              `(-width_shift_range, +width_shift_range)`\n",
      " |          - With `width_shift_range=2` possible values\n",
      " |              are integers `[-1, 0, +1]`,\n",
      " |              same as with `width_shift_range=[-1, 0, +1]`,\n",
      " |              while with `width_shift_range=1.0` possible values are floats\n",
      " |              in the interval [-1.0, +1.0).\n",
      " |      height_shift_range: Float, 1-D array-like or int\n",
      " |          - float: fraction of total height, if < 1, or pixels if >= 1.\n",
      " |          - 1-D array-like: random elements from the array.\n",
      " |          - int: integer number of pixels from interval\n",
      " |              `(-height_shift_range, +height_shift_range)`\n",
      " |          - With `height_shift_range=2` possible values\n",
      " |              are integers `[-1, 0, +1]`,\n",
      " |              same as with `height_shift_range=[-1, 0, +1]`,\n",
      " |              while with `height_shift_range=1.0` possible values are floats\n",
      " |              in the interval [-1.0, +1.0).\n",
      " |      brightness_range: Tuple or list of two floats. Range for picking\n",
      " |          a brightness shift value from.\n",
      " |      shear_range: Float. Shear Intensity\n",
      " |          (Shear angle in counter-clockwise direction in degrees)\n",
      " |      zoom_range: Float or [lower, upper]. Range for random zoom.\n",
      " |          If a float, `[lower, upper] = [1-zoom_range, 1+zoom_range]`.\n",
      " |      channel_shift_range: Float. Range for random channel shifts.\n",
      " |      fill_mode: One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}.\n",
      " |          Default is 'nearest'.\n",
      " |          Points outside the boundaries of the input are filled\n",
      " |          according to the given mode:\n",
      " |          - 'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k)\n",
      " |          - 'nearest':  aaaaaaaa|abcd|dddddddd\n",
      " |          - 'reflect':  abcddcba|abcd|dcbaabcd\n",
      " |          - 'wrap':  abcdabcd|abcd|abcdabcd\n",
      " |      cval: Float or Int.\n",
      " |          Value used for points outside the boundaries\n",
      " |          when `fill_mode = \"constant\"`.\n",
      " |      horizontal_flip: Boolean. Randomly flip inputs horizontally.\n",
      " |      vertical_flip: Boolean. Randomly flip inputs vertically.\n",
      " |      rescale: rescaling factor. Defaults to None.\n",
      " |          If None or 0, no rescaling is applied,\n",
      " |          otherwise we multiply the data by the value provided\n",
      " |          (after applying all other transformations).\n",
      " |      preprocessing_function: function that will be applied on each input.\n",
      " |          The function will run after the image is resized and augmented.\n",
      " |          The function should take one argument:\n",
      " |          one image (Numpy tensor with rank 3),\n",
      " |          and should output a Numpy tensor with the same shape.\n",
      " |      data_format: Image data format,\n",
      " |          either \"channels_first\" or \"channels_last\".\n",
      " |          \"channels_last\" mode means that the images should have shape\n",
      " |          `(samples, height, width, channels)`,\n",
      " |          \"channels_first\" mode means that the images should have shape\n",
      " |          `(samples, channels, height, width)`.\n",
      " |          It defaults to the `image_data_format` value found in your\n",
      " |          Keras config file at `~/.keras/keras.json`.\n",
      " |          If you never set it, then it will be \"channels_last\".\n",
      " |      validation_split: Float. Fraction of images reserved for validation\n",
      " |          (strictly between 0 and 1).\n",
      " |      dtype: Dtype to use for the generated arrays.\n",
      " |  \n",
      " |  Examples:\n",
      " |  \n",
      " |  Example of using `.flow(x, y)`:\n",
      " |  \n",
      " |  ```python\n",
      " |  (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
      " |  y_train = np_utils.to_categorical(y_train, num_classes)\n",
      " |  y_test = np_utils.to_categorical(y_test, num_classes)\n",
      " |  datagen = ImageDataGenerator(\n",
      " |      featurewise_center=True,\n",
      " |      featurewise_std_normalization=True,\n",
      " |      rotation_range=20,\n",
      " |      width_shift_range=0.2,\n",
      " |      height_shift_range=0.2,\n",
      " |      horizontal_flip=True)\n",
      " |  # compute quantities required for featurewise normalization\n",
      " |  # (std, mean, and principal components if ZCA whitening is applied)\n",
      " |  datagen.fit(x_train)\n",
      " |  # fits the model on batches with real-time data augmentation:\n",
      " |  model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
      " |                      steps_per_epoch=len(x_train) / 32, epochs=epochs)\n",
      " |  # here's a more \"manual\" example\n",
      " |  for e in range(epochs):\n",
      " |      print('Epoch', e)\n",
      " |      batches = 0\n",
      " |      for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):\n",
      " |          model.fit(x_batch, y_batch)\n",
      " |          batches += 1\n",
      " |          if batches >= len(x_train) / 32:\n",
      " |              # we need to break the loop by hand because\n",
      " |              # the generator loops indefinitely\n",
      " |              break\n",
      " |  ```\n",
      " |  \n",
      " |  Example of using `.flow_from_directory(directory)`:\n",
      " |  \n",
      " |  ```python\n",
      " |  train_datagen = ImageDataGenerator(\n",
      " |          rescale=1./255,\n",
      " |          shear_range=0.2,\n",
      " |          zoom_range=0.2,\n",
      " |          horizontal_flip=True)\n",
      " |  test_datagen = ImageDataGenerator(rescale=1./255)\n",
      " |  train_generator = train_datagen.flow_from_directory(\n",
      " |          'data/train',\n",
      " |          target_size=(150, 150),\n",
      " |          batch_size=32,\n",
      " |          class_mode='binary')\n",
      " |  validation_generator = test_datagen.flow_from_directory(\n",
      " |          'data/validation',\n",
      " |          target_size=(150, 150),\n",
      " |          batch_size=32,\n",
      " |          class_mode='binary')\n",
      " |  model.fit_generator(\n",
      " |          train_generator,\n",
      " |          steps_per_epoch=2000,\n",
      " |          epochs=50,\n",
      " |          validation_data=validation_generator,\n",
      " |          validation_steps=800)\n",
      " |  ```\n",
      " |  \n",
      " |  Example of transforming images and masks together.\n",
      " |  \n",
      " |  ```python\n",
      " |  # we create two instances with the same arguments\n",
      " |  data_gen_args = dict(featurewise_center=True,\n",
      " |                       featurewise_std_normalization=True,\n",
      " |                       rotation_range=90,\n",
      " |                       width_shift_range=0.1,\n",
      " |                       height_shift_range=0.1,\n",
      " |                       zoom_range=0.2)\n",
      " |  image_datagen = ImageDataGenerator(**data_gen_args)\n",
      " |  mask_datagen = ImageDataGenerator(**data_gen_args)\n",
      " |  # Provide the same seed and keyword arguments to the fit and flow methods\n",
      " |  seed = 1\n",
      " |  image_datagen.fit(images, augment=True, seed=seed)\n",
      " |  mask_datagen.fit(masks, augment=True, seed=seed)\n",
      " |  image_generator = image_datagen.flow_from_directory(\n",
      " |      'data/images',\n",
      " |      class_mode=None,\n",
      " |      seed=seed)\n",
      " |  mask_generator = mask_datagen.flow_from_directory(\n",
      " |      'data/masks',\n",
      " |      class_mode=None,\n",
      " |      seed=seed)\n",
      " |  # combine generators into one which yields image and masks\n",
      " |  train_generator = zip(image_generator, mask_generator)\n",
      " |  model.fit_generator(\n",
      " |      train_generator,\n",
      " |      steps_per_epoch=2000,\n",
      " |      epochs=50)\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ImageDataGenerator\n",
      " |      keras_preprocessing.image.image_data_generator.ImageDataGenerator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0, height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0, channel_shift_range=0.0, fill_mode='nearest', cval=0.0, horizontal_flip=False, vertical_flip=False, rescale=None, preprocessing_function=None, data_format=None, validation_split=0.0, dtype=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras_preprocessing.image.image_data_generator.ImageDataGenerator:\n",
      " |  \n",
      " |  apply_transform(self, x, transform_parameters)\n",
      " |      Applies a transformation to an image according to given parameters.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: 3D tensor, single image.\n",
      " |          transform_parameters: Dictionary with string - parameter pairs\n",
      " |              describing the transformation.\n",
      " |              Currently, the following parameters\n",
      " |              from the dictionary are used:\n",
      " |              - `'theta'`: Float. Rotation angle in degrees.\n",
      " |              - `'tx'`: Float. Shift in the x direction.\n",
      " |              - `'ty'`: Float. Shift in the y direction.\n",
      " |              - `'shear'`: Float. Shear angle in degrees.\n",
      " |              - `'zx'`: Float. Zoom in the x direction.\n",
      " |              - `'zy'`: Float. Zoom in the y direction.\n",
      " |              - `'flip_horizontal'`: Boolean. Horizontal flip.\n",
      " |              - `'flip_vertical'`: Boolean. Vertical flip.\n",
      " |              - `'channel_shift_intencity'`: Float. Channel shift intensity.\n",
      " |              - `'brightness'`: Float. Brightness shift intensity.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A transformed version of the input (same shape).\n",
      " |  \n",
      " |  fit(self, x, augment=False, rounds=1, seed=None)\n",
      " |      Fits the data generator to some sample data.\n",
      " |      \n",
      " |      This computes the internal data stats related to the\n",
      " |      data-dependent transformations, based on an array of sample data.\n",
      " |      \n",
      " |      Only required if `featurewise_center` or\n",
      " |      `featurewise_std_normalization` or `zca_whitening` are set to True.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: Sample data. Should have rank 4.\n",
      " |           In case of grayscale data,\n",
      " |           the channels axis should have value 1, in case\n",
      " |           of RGB data, it should have value 3, and in case\n",
      " |           of RGBA data, it should have value 4.\n",
      " |          augment: Boolean (default: False).\n",
      " |              Whether to fit on randomly augmented samples.\n",
      " |          rounds: Int (default: 1).\n",
      " |              If using data augmentation (`augment=True`),\n",
      " |              this is how many augmentation passes over the data to use.\n",
      " |          seed: Int (default: None). Random seed.\n",
      " |  \n",
      " |  flow(self, x, y=None, batch_size=32, shuffle=True, sample_weight=None, seed=None, save_to_dir=None, save_prefix='', save_format='png', subset=None)\n",
      " |      Takes data & label arrays, generates batches of augmented data.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: Input data. Numpy array of rank 4 or a tuple.\n",
      " |              If tuple, the first element\n",
      " |              should contain the images and the second element\n",
      " |              another numpy array or a list of numpy arrays\n",
      " |              that gets passed to the output\n",
      " |              without any modifications.\n",
      " |              Can be used to feed the model miscellaneous data\n",
      " |              along with the images.\n",
      " |              In case of grayscale data, the channels axis of the image array\n",
      " |              should have value 1, in case\n",
      " |              of RGB data, it should have value 3, and in case\n",
      " |              of RGBA data, it should have value 4.\n",
      " |          y: Labels.\n",
      " |          batch_size: Int (default: 32).\n",
      " |          shuffle: Boolean (default: True).\n",
      " |          sample_weight: Sample weights.\n",
      " |          seed: Int (default: None).\n",
      " |          save_to_dir: None or str (default: None).\n",
      " |              This allows you to optionally specify a directory\n",
      " |              to which to save the augmented pictures being generated\n",
      " |              (useful for visualizing what you are doing).\n",
      " |          save_prefix: Str (default: `''`).\n",
      " |              Prefix to use for filenames of saved pictures\n",
      " |              (only relevant if `save_to_dir` is set).\n",
      " |          save_format: one of \"png\", \"jpeg\"\n",
      " |              (only relevant if `save_to_dir` is set). Default: \"png\".\n",
      " |          subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
      " |              `validation_split` is set in `ImageDataGenerator`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An `Iterator` yielding tuples of `(x, y)`\n",
      " |              where `x` is a numpy array of image data\n",
      " |              (in the case of a single image input) or a list\n",
      " |              of numpy arrays (in the case with\n",
      " |              additional inputs) and `y` is a numpy array\n",
      " |              of corresponding labels. If 'sample_weight' is not None,\n",
      " |              the yielded tuples are of the form `(x, y, sample_weight)`.\n",
      " |              If `y` is None, only the numpy array `x` is returned.\n",
      " |  \n",
      " |  flow_from_dataframe(self, dataframe, directory=None, x_col='filename', y_col='class', weight_col=None, target_size=(256, 256), color_mode='rgb', classes=None, class_mode='categorical', batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix='', save_format='png', subset=None, interpolation='nearest', validate_filenames=True, **kwargs)\n",
      " |      Takes the dataframe and the path to a directory\n",
      " |       and generates batches of augmented/normalized data.\n",
      " |      \n",
      " |      **A simple tutorial can be found **[here](\n",
      " |                                  http://bit.ly/keras_flow_from_dataframe).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          dataframe: Pandas dataframe containing the filepaths relative to\n",
      " |              `directory` (or absolute paths if `directory` is None) of the\n",
      " |              images in a string column. It should include other column/s\n",
      " |              depending on the `class_mode`:\n",
      " |              - if `class_mode` is `\"categorical\"` (default value) it must\n",
      " |                  include the `y_col` column with the class/es of each image.\n",
      " |                  Values in column can be string/list/tuple if a single class\n",
      " |                  or list/tuple if multiple classes.\n",
      " |              - if `class_mode` is `\"binary\"` or `\"sparse\"` it must include\n",
      " |                  the given `y_col` column with class values as strings.\n",
      " |              - if `class_mode` is `\"raw\"` or `\"multi_output\"` it should contain\n",
      " |              the columns specified in `y_col`.\n",
      " |              - if `class_mode` is `\"input\"` or `None` no extra column is needed.\n",
      " |          directory: string, path to the directory to read images from. If `None`,\n",
      " |              data in `x_col` column should be absolute paths.\n",
      " |          x_col: string, column in `dataframe` that contains the filenames (or\n",
      " |              absolute paths if `directory` is `None`).\n",
      " |          y_col: string or list, column/s in `dataframe` that has the target data.\n",
      " |          weight_col: string, column in `dataframe` that contains the sample\n",
      " |              weights. Default: `None`.\n",
      " |          target_size: tuple of integers `(height, width)`, default: `(256, 256)`.\n",
      " |              The dimensions to which all images found will be resized.\n",
      " |          color_mode: one of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".\n",
      " |              Whether the images will be converted to have 1 or 3 color channels.\n",
      " |          classes: optional list of classes (e.g. `['dogs', 'cats']`).\n",
      " |              Default: None. If not provided, the list of classes will be\n",
      " |              automatically inferred from the `y_col`,\n",
      " |              which will map to the label indices, will be alphanumeric).\n",
      " |              The dictionary containing the mapping from class names to class\n",
      " |              indices can be obtained via the attribute `class_indices`.\n",
      " |          class_mode: one of \"binary\", \"categorical\", \"input\", \"multi_output\",\n",
      " |              \"raw\", sparse\" or None. Default: \"categorical\".\n",
      " |              Mode for yielding the targets:\n",
      " |              - `\"binary\"`: 1D numpy array of binary labels,\n",
      " |              - `\"categorical\"`: 2D numpy array of one-hot encoded labels.\n",
      " |                  Supports multi-label output.\n",
      " |              - `\"input\"`: images identical to input images (mainly used to\n",
      " |                  work with autoencoders),\n",
      " |              - `\"multi_output\"`: list with the values of the different columns,\n",
      " |              - `\"raw\"`: numpy array of values in `y_col` column(s),\n",
      " |              - `\"sparse\"`: 1D numpy array of integer labels,\n",
      " |              - `None`, no targets are returned (the generator will only yield\n",
      " |                  batches of image data, which is useful to use in\n",
      " |                  `model.predict_generator()`).\n",
      " |          batch_size: size of the batches of data (default: 32).\n",
      " |          shuffle: whether to shuffle the data (default: True)\n",
      " |          seed: optional random seed for shuffling and transformations.\n",
      " |          save_to_dir: None or str (default: None).\n",
      " |              This allows you to optionally specify a directory\n",
      " |              to which to save the augmented pictures being generated\n",
      " |              (useful for visualizing what you are doing).\n",
      " |          save_prefix: str. Prefix to use for filenames of saved pictures\n",
      " |              (only relevant if `save_to_dir` is set).\n",
      " |          save_format: one of \"png\", \"jpeg\"\n",
      " |              (only relevant if `save_to_dir` is set). Default: \"png\".\n",
      " |          follow_links: whether to follow symlinks inside class subdirectories\n",
      " |              (default: False).\n",
      " |          subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
      " |              `validation_split` is set in `ImageDataGenerator`.\n",
      " |          interpolation: Interpolation method used to resample the image if the\n",
      " |              target size is different from that of the loaded image.\n",
      " |              Supported methods are `\"nearest\"`, `\"bilinear\"`, and `\"bicubic\"`.\n",
      " |              If PIL version 1.1.3 or newer is installed, `\"lanczos\"` is also\n",
      " |              supported. If PIL version 3.4.0 or newer is installed, `\"box\"` and\n",
      " |              `\"hamming\"` are also supported. By default, `\"nearest\"` is used.\n",
      " |          validate_filenames: Boolean, whether to validate image filenames in\n",
      " |              `x_col`. If `True`, invalid images will be ignored. Disabling this\n",
      " |              option can lead to speed-up in the execution of this function.\n",
      " |              Default: `True`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A `DataFrameIterator` yielding tuples of `(x, y)`\n",
      " |          where `x` is a numpy array containing a batch\n",
      " |          of images with shape `(batch_size, *target_size, channels)`\n",
      " |          and `y` is a numpy array of corresponding labels.\n",
      " |  \n",
      " |  flow_from_directory(self, directory, target_size=(256, 256), color_mode='rgb', classes=None, class_mode='categorical', batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix='', save_format='png', follow_links=False, subset=None, interpolation='nearest')\n",
      " |      Takes the path to a directory & generates batches of augmented data.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          directory: string, path to the target directory.\n",
      " |              It should contain one subdirectory per class.\n",
      " |              Any PNG, JPG, BMP, PPM or TIF images\n",
      " |              inside each of the subdirectories directory tree\n",
      " |              will be included in the generator.\n",
      " |              See [this script](\n",
      " |              https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d)\n",
      " |              for more details.\n",
      " |          target_size: Tuple of integers `(height, width)`,\n",
      " |              default: `(256, 256)`.\n",
      " |              The dimensions to which all images found will be resized.\n",
      " |          color_mode: One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".\n",
      " |              Whether the images will be converted to\n",
      " |              have 1, 3, or 4 channels.\n",
      " |          classes: Optional list of class subdirectories\n",
      " |              (e.g. `['dogs', 'cats']`). Default: None.\n",
      " |              If not provided, the list of classes will be automatically\n",
      " |              inferred from the subdirectory names/structure\n",
      " |              under `directory`, where each subdirectory will\n",
      " |              be treated as a different class\n",
      " |              (and the order of the classes, which will map to the label\n",
      " |              indices, will be alphanumeric).\n",
      " |              The dictionary containing the mapping from class names to class\n",
      " |              indices can be obtained via the attribute `class_indices`.\n",
      " |          class_mode: One of \"categorical\", \"binary\", \"sparse\",\n",
      " |              \"input\", or None. Default: \"categorical\".\n",
      " |              Determines the type of label arrays that are returned:\n",
      " |              - \"categorical\" will be 2D one-hot encoded labels,\n",
      " |              - \"binary\" will be 1D binary labels,\n",
      " |                  \"sparse\" will be 1D integer labels,\n",
      " |              - \"input\" will be images identical\n",
      " |                  to input images (mainly used to work with autoencoders).\n",
      " |              - If None, no labels are returned\n",
      " |                (the generator will only yield batches of image data,\n",
      " |                which is useful to use with `model.predict_generator()`).\n",
      " |                Please note that in case of class_mode None,\n",
      " |                the data still needs to reside in a subdirectory\n",
      " |                of `directory` for it to work correctly.\n",
      " |          batch_size: Size of the batches of data (default: 32).\n",
      " |          shuffle: Whether to shuffle the data (default: True)\n",
      " |              If set to False, sorts the data in alphanumeric order.\n",
      " |          seed: Optional random seed for shuffling and transformations.\n",
      " |          save_to_dir: None or str (default: None).\n",
      " |              This allows you to optionally specify\n",
      " |              a directory to which to save\n",
      " |              the augmented pictures being generated\n",
      " |              (useful for visualizing what you are doing).\n",
      " |          save_prefix: Str. Prefix to use for filenames of saved pictures\n",
      " |              (only relevant if `save_to_dir` is set).\n",
      " |          save_format: One of \"png\", \"jpeg\"\n",
      " |              (only relevant if `save_to_dir` is set). Default: \"png\".\n",
      " |          follow_links: Whether to follow symlinks inside\n",
      " |              class subdirectories (default: False).\n",
      " |          subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
      " |              `validation_split` is set in `ImageDataGenerator`.\n",
      " |          interpolation: Interpolation method used to\n",
      " |              resample the image if the\n",
      " |              target size is different from that of the loaded image.\n",
      " |              Supported methods are `\"nearest\"`, `\"bilinear\"`,\n",
      " |              and `\"bicubic\"`.\n",
      " |              If PIL version 1.1.3 or newer is installed, `\"lanczos\"` is also\n",
      " |              supported. If PIL version 3.4.0 or newer is installed,\n",
      " |              `\"box\"` and `\"hamming\"` are also supported.\n",
      " |              By default, `\"nearest\"` is used.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A `DirectoryIterator` yielding tuples of `(x, y)`\n",
      " |              where `x` is a numpy array containing a batch\n",
      " |              of images with shape `(batch_size, *target_size, channels)`\n",
      " |              and `y` is a numpy array of corresponding labels.\n",
      " |  \n",
      " |  get_random_transform(self, img_shape, seed=None)\n",
      " |      Generates random parameters for a transformation.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          seed: Random seed.\n",
      " |          img_shape: Tuple of integers.\n",
      " |              Shape of the image that is transformed.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A dictionary containing randomly chosen parameters describing the\n",
      " |          transformation.\n",
      " |  \n",
      " |  random_transform(self, x, seed=None)\n",
      " |      Applies a random transformation to an image.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: 3D tensor, single image.\n",
      " |          seed: Random seed.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A randomly transformed version of the input (same shape).\n",
      " |  \n",
      " |  standardize(self, x)\n",
      " |      Applies the normalization configuration in-place to a batch of inputs.\n",
      " |      \n",
      " |      `x` is changed in-place since the function is mainly used internally\n",
      " |      to standarize images and feed them to your network. If a copy of `x`\n",
      " |      would be created instead it would have a significant performance cost.\n",
      " |      If you want to apply this method without changing the input in-place\n",
      " |      you can call the method creating a copy before:\n",
      " |      \n",
      " |      standarize(np.copy(x))\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: Batch of inputs to be normalized.\n",
      " |      \n",
      " |      # Returns\n",
      " |          The inputs, normalized.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras_preprocessing.image.image_data_generator.ImageDataGenerator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ImageDataGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method flow in module keras_preprocessing.image.image_data_generator:\n",
      "\n",
      "flow(x, y=None, batch_size=32, shuffle=True, sample_weight=None, seed=None, save_to_dir=None, save_prefix='', save_format='png', subset=None) method of tensorflow.python.keras.preprocessing.image.ImageDataGenerator instance\n",
      "    Takes data & label arrays, generates batches of augmented data.\n",
      "    \n",
      "    # Arguments\n",
      "        x: Input data. Numpy array of rank 4 or a tuple.\n",
      "            If tuple, the first element\n",
      "            should contain the images and the second element\n",
      "            another numpy array or a list of numpy arrays\n",
      "            that gets passed to the output\n",
      "            without any modifications.\n",
      "            Can be used to feed the model miscellaneous data\n",
      "            along with the images.\n",
      "            In case of grayscale data, the channels axis of the image array\n",
      "            should have value 1, in case\n",
      "            of RGB data, it should have value 3, and in case\n",
      "            of RGBA data, it should have value 4.\n",
      "        y: Labels.\n",
      "        batch_size: Int (default: 32).\n",
      "        shuffle: Boolean (default: True).\n",
      "        sample_weight: Sample weights.\n",
      "        seed: Int (default: None).\n",
      "        save_to_dir: None or str (default: None).\n",
      "            This allows you to optionally specify a directory\n",
      "            to which to save the augmented pictures being generated\n",
      "            (useful for visualizing what you are doing).\n",
      "        save_prefix: Str (default: `''`).\n",
      "            Prefix to use for filenames of saved pictures\n",
      "            (only relevant if `save_to_dir` is set).\n",
      "        save_format: one of \"png\", \"jpeg\"\n",
      "            (only relevant if `save_to_dir` is set). Default: \"png\".\n",
      "        subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
      "            `validation_split` is set in `ImageDataGenerator`.\n",
      "    \n",
      "    # Returns\n",
      "        An `Iterator` yielding tuples of `(x, y)`\n",
      "            where `x` is a numpy array of image data\n",
      "            (in the case of a single image input) or a list\n",
      "            of numpy arrays (in the case with\n",
      "            additional inputs) and `y` is a numpy array\n",
      "            of corresponding labels. If 'sample_weight' is not None,\n",
      "            the yielded tuples are of the form `(x, y, sample_weight)`.\n",
      "            If `y` is None, only the numpy array `x` is returned.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(data_gen.flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = data_gen.flow(X_train,y_train,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = X_train.shape[0]//32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-57-a2a3b00b971c>:1: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/20\n",
      "1562/1562 [==============================] - 34s 22ms/step - loss: 1.2524 - accuracy: 0.5701\n",
      "Epoch 2/20\n",
      "1562/1562 [==============================] - 34s 22ms/step - loss: 1.1415 - accuracy: 0.6014\n",
      "Epoch 3/20\n",
      "1562/1562 [==============================] - 34s 22ms/step - loss: 1.0976 - accuracy: 0.6160\n",
      "Epoch 4/20\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.0613 - accuracy: 0.6290\n",
      "Epoch 5/20\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.0355 - accuracy: 0.6379\n",
      "Epoch 6/20\n",
      "1562/1562 [==============================] - 33s 21ms/step - loss: 1.0075 - accuracy: 0.6481\n",
      "Epoch 7/20\n",
      "1562/1562 [==============================] - 33s 21ms/step - loss: 0.9987 - accuracy: 0.6514\n",
      "Epoch 8/20\n",
      "1562/1562 [==============================] - 33s 21ms/step - loss: 0.9809 - accuracy: 0.6589\n",
      "Epoch 9/20\n",
      "1562/1562 [==============================] - 34s 22ms/step - loss: 0.9649 - accuracy: 0.6626\n",
      "Epoch 10/20\n",
      "1562/1562 [==============================] - 34s 22ms/step - loss: 0.9537 - accuracy: 0.6681\n",
      "Epoch 11/20\n",
      "1562/1562 [==============================] - 44s 28ms/step - loss: 0.9389 - accuracy: 0.6719\n",
      "Epoch 12/20\n",
      "1562/1562 [==============================] - 51s 32ms/step - loss: 0.9292 - accuracy: 0.6765\n",
      "Epoch 13/20\n",
      "1562/1562 [==============================] - 48s 31ms/step - loss: 0.9212 - accuracy: 0.6780\n",
      "Epoch 14/20\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 0.9169 - accuracy: 0.6796\n",
      "Epoch 15/20\n",
      "1562/1562 [==============================] - 47s 30ms/step - loss: 0.9114 - accuracy: 0.6828\n",
      "Epoch 16/20\n",
      "1562/1562 [==============================] - 48s 30ms/step - loss: 0.8923 - accuracy: 0.6873\n",
      "Epoch 17/20\n",
      "1562/1562 [==============================] - 42s 27ms/step - loss: 0.8894 - accuracy: 0.6889\n",
      "Epoch 18/20\n",
      "1562/1562 [==============================] - 51s 32ms/step - loss: 0.8830 - accuracy: 0.6925\n",
      "Epoch 19/20\n",
      "1562/1562 [==============================] - 44s 28ms/step - loss: 0.8722 - accuracy: 0.6955\n",
      "Epoch 20/20\n",
      "1562/1562 [==============================] - 37s 23ms/step - loss: 0.8705 - accuracy: 0.6985\n"
     ]
    }
   ],
   "source": [
    "r = model.fit_generator(train_gen , steps_per_epoch = steps_per_epoch ,epochs = 20 , callbacks=[callbacks])  # can now use model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
